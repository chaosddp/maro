# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

env:
  scenario: supply_chain
  # Currently available topologies are "sample1" or "random". New topologies must consist of a single folder
  # that contains a single config.yml and shoud be placed under examples/supply_chain/envs/
  topology: test
  durations: 64  # number of ticks per episode
num_episodes: 1000  # number of episodes to simulate
# Number of roll-out steps in each learning cycle. Each actor will perform at most this many roll-out steps
# before returning experiences to the learner. The learner uses these experiences to update the agents' policies
# and sync the updated policies to the actors for the next learning cycle.
experience_update_interval: 64
eval_schedule: 100
log_env_metrics: false
policy:
  consumerstore:
    algorithm: dqn
    model:  # Edit the get_dqn_agent() code in examples\supply_chain\agent.py if you need to customize the model.
      device: cpu
      network:
        hidden_dims:
          - 256
          - 128
          - 32
        output_dim: 10
        activation: leaky_relu  # refer to maro/maro/rl/utils/torch_cls_index.py for the mapping of strings to torch activation classes.
        softmax: true
        batch_norm: false
        skip_connection: false
        head: true
        dropout_p: 0.0
      optimization:
        optim_cls: adam  # refer to maro/maro/rl/utils/torch_cls_index.py for the mapping of strings to torch optimizer classes.
        optim_params:
          lr: 0.0005
    algorithm_config:
      reward_discount: .99
      train_epochs: 10
      gradient_iters: 1
      loss_cls: mse  # refer to maro/maro/rl/utils/torch_cls_index.py for the mapping of strings to torch loss classes.
      target_update_freq: 4  # How many training iteration, to update DQN target model
      soft_update_coefficient: 0.01
      double: true   # whether to enable double DQN
    experience_manager:
      capacity: 128000  # experience memory size
      # This determines how existing experiences are replaced when adding new experiences to a full experience
      # memory. Must be one of "rolling" or "random". If "rolling", experiences will be replaced sequentially,
      # with the oldest one being the first to be replaced. If "random", experiences will be replaced randomly.
      overwrite_type: rolling
      batch_size: 2560
      replace: true
    update_schedule:
      type: step  # "step" or "episode"
      args:
        start_ep: 3   # must be a positive number since episode is 1-based.
        interval: 1
        end_ep_update: true
  consumer:
    model:  # Edit the get_dqn_agent() code in examples\supply_chain\agent.py if you need to customize the model.
      network:
        output_dim: 10
  producer:
    model:  # Edit the get_dqn_agent() code in examples\supply_chain\agent.py if you need to customize the model.
      network:
        output_dim: 10
exploration:
  last_ep: 800
  initial_value: 0.8 # Here (start: 0.4, end: 0.0) means: the exploration rate will start at 0.4 and decrease linearly to 0.0 in the last episode.
  final_value: 0.0
distributed:
  # this is used to group all actor / learner processes belonging to the same job for communication purposes.
  # There is no need to change this if you use the scripts under examples/supply_chain/scripts to run the scenario.
  group: sc-dqn
  num_actors: 3  # number of parallel roll-out actors
  # If you use the scripts under examples/supply_chain/scripts to run the scenario, you can set "redis_host"
  # to any string supported by the pyyaml parser. If running in multi-process mode, change this to "localhost" and make
  # sure that a local redis server is running and listening on the port specified by "redis_port".
  redis_host: maro-redis
  redis_port: 6379
  # The number of actor finishes required for the learner to enter the next learning cycle. This is used to prevent
  # slow actors from dragging down the whole process.
  required_actor_finishes: 3
  # If true, experiences from older segments (usually coming from slow actors) will not be used for learning.
  discard_stale_experiences: True
